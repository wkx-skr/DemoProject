spring:
  jackson:
    serialization:
      write-dates-as-timestamps: true
  redis:
    redisson:
      config: ${datablau.redisson.config.location}
  autoconfigure:
    exclude: com.alibaba.boot.nacos.discovery.autoconfigure.NacosDiscoveryAutoConfiguration, org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration, org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration, org.springframework.cloud.netflix.eureka.EurekaDiscoveryClientConfiguration, org.springframework.boot.actuate.autoconfigure.health.HealthContributorAutoConfiguration
  servlet:
    multipart:
      max-file-size: 1024MB
      max-request-size: 1024MB
  cloud:
    config:
      enabled: false
  main:
    allow-circular-references: true
  kafka:
    # docker http://192.168.2.202:8080
    bootstrap-servers: ${common.kafka.bootstrap-servers}
    producer:
      retries: 3
      acks: 1
      batch-size: 16384
      properties:
        sasl:
          mechanism: PLAIN
          jaas:
            config: org.apache.kafka.common.security.plain.PlainLoginModule required username="${common.kafka.n}" password="${common.kafka.w}";
        security:
          protocol: SASL_PLAINTEXT
        mechanism: PLAIN
        jaas:
          config: org.apache.kafka.common.security.plain.PlainLoginModule required username="${common.kafka.n}" password="${common.kafka.w}";
        linger:
          ms: 100
      buffer-memory: 33554432
      # key的序列化类
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # value的序列化类
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      # 生产者生成的所有数据的压缩类型，此配置接受标准压缩编解码器（'gzip'，'snappy'，'lz4'，'zstd'）
      # 默认为none
      compression-type: none
    consumer:
      group-id: metadata_datablau_dataquality
      enable-auto-commit: false
      auto-offset-reset: latest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        sasl:
          mechanism: PLAIN
          jaas:
            config: org.apache.kafka.common.security.plain.PlainLoginModule required username="${common.kafka.n}" password="${common.kafka.w}";
        security:
          protocol: SASL_PLAINTEXT
        spring:
          json:
            trusted:
              packages: "com.datablau.*"
main:
  banner-mode: console

aop:
  proxy-target-class: true

server:
  port: ${datablau.server.port}
  address: 0.0.0.0
  servlet:
    context-path: /quality
  error:
    include-stacktrace: always

logging:
  config: classpath:log4j2-spring.xml
  file:
    path: logs
  level:
    com.netflix: WARN
    com.datablau: INFO
    com.alibaba.nacos.client: ERROR
    com.alibaba.nacos.common.remote.client: ERROR
    org:
      springframework:
        session: error
        boot:
          autoconfigure:
            security: WARN
  pattern:
    file: "[%d{dd/MM/yy HH:mm:ss:sss z}] %-5p [%t](%c{1.}:%L) - %m%n"

datablau:
  server:
    port: ${spring.cloud.nacos.discovery.port} #读取bootstrap中设置的ip
    address: ${spring.cloud.nacos.discovery.ip} #读取bootstrap中设置的port
    build:
      version: 7.0.0
  db:
    url: ${common.db.url}
    driver-class-name: ${common.db.driver-class-name}
    ip-address: ${common.db.ip-address}
    port: ${common.db.port}
    target: dam_base
    parameters: ${common.db.parameters}
    username: ${common.db.username}
    password: ${common.db.password}
    dialect: ${common.db.dialect}
    hibernate:
      hbm2ddl: update
    max-total: 50
    min-idle: 2
    max-wait-millis: 30000
  redisson:
    config:
      location: nacos-data-id:redisson.yaml
  service:
    port: ${spring.cloud.nacos.discovery.port}
    call-port: 6111
  transport:
    #是否使用httpInvoker，如果为false或者不存在，则采用rmi
    http: true
  web-instance:
    name: datablau-dataquality-web
    enable: true
  #swagger-ui 开启配置 (true: 开启; false: 关闭)
  swagger-ui-open: false
  #是否启用kafka
  kafka:
    enable: false
    system-log:
      enable: true #服务系统日志
  plugins:
    datasource:
      path: d:/plugins
  job:
    file: d:/test.jar
  kafka-topic:
    #消费者
    #删除数据源
    model-delete: ${common.kafka.topic.metadata.model-delete}
    #系统日志
    audit-system-log: ${common.kafka.topic.audit.system-log}
    rule-update: ${common.kafka.topic.base.rule-update}
    audit-common-log: ${common.kafka.topic.audit.log}
    entity-change-event: ${common.kafka.topic.entity-change-event}

quality:
  result:
    es:
      enable: false

transform:
  input:
    buffer:
      #使用bufferedReader时，从数据库最多读取多少行缓存
      size: 1000
  buffered:
    max:
      #使用bufferedReader时，从数据库最多读取多少个对象缓存
      cells: 900000
    dbfile:
      # When doing full data merge join, temp db file will be created, this value indicates how long the db file will be kept and be reuesed, defaults to 1hour, the unit is second
      alive:
  temp:
    file:
      #当做ETL的时候，有时候需要创建临时文件，这个配置选择临时文件目录，如果不设置那么就是%tmp%
      path:
